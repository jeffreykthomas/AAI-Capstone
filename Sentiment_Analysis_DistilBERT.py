# -*- coding: utf-8 -*-
"""Sentiment_Analysis_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18aJtA595pOTbrK8WMQPdDtJWNM-_S2Bc
"""

import spacy
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from google.colab import drive
import requests
import os
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from transformers import DistilBertTokenizer, AutoTokenizer

df_preprocessed = pd.read_csv('processed.csv')
df_preprocessed.head(10)

model_ckpt = "distilbert-base-uncased"
tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)
tokenizer.model_input_names

df_preprocessed['lemmas'] = df_preprocessed['lemmas'].astype(str)

length = df_preprocessed['tokens'].apply(len)
print(length.describe())

plt.hist(length, bins=50, alpha=0.7)
plt.title('Distribution of Sequence Lengths')
plt.xlabel('Sequence Length')
plt.ylabel('Frequency')
plt.show()

def tokenize(text, max_length=200):
  return tokenizer(text, padding='max_length', truncation=True)

print(tokenize(df_preprocessed['lemmas'][5]))

df_preprocessed['input_ids'] = None
df_preprocessed['attention_mask'] = None

for index, row in df_preprocessed.iterrows():
    tokenized_output = tokenize(row['lemmas'])
    df_preprocessed.at[index, 'input_ids'] = tokenized_output['input_ids']
    df_preprocessed.at[index, 'attention_mask'] = tokenized_output['attention_mask']

df_preprocessed.head(10)

label_list = df_preprocessed['emotion_label'].unique()
num_labels = len(label_list)
print('Labels: ',label_list, '\n Qty:', num_labels)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(df_preprocessed['emotion_label'])

emotions_encoded = pd.DataFrame(columns=['attention_mask', 'input_ids', 'label',
                                         'text' ])
emotions_encoded['attention_mask'] = df_preprocessed['attention_mask']
emotions_encoded['input_ids'] = df_preprocessed['input_ids']
emotions_encoded['label'] = labels
emotions_encoded['text'] = df_preprocessed['cleaned_text']

emotions_encoded

from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(
    emotions_encoded,
    test_size=0.2,
    stratify=emotions_encoded['label'],
    random_state=42
)

import torch
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification

input_ids = torch.tensor(train_df['input_ids'].tolist())
attention_mask = torch.tensor(train_df['attention_mask'].tolist())
labels = torch.tensor(train_df['label'].tolist())

from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, input_ids, attention_mask, labels):
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': self.labels[idx]
        }

train_dataset = CustomDataset(input_ids, attention_mask, labels)

val_input_ids = torch.tensor(test_df['input_ids'].tolist())
val_attention_mask = torch.tensor(test_df['attention_mask'].tolist())
val_labels = torch.tensor(test_df['label'].tolist())

val_dataset = CustomDataset(val_input_ids, val_attention_mask, val_labels)

device = torch.device("cpu")
model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device))

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels, preds, average="weighted")
  acc = accuracy_score(labels, preds)
  return {"accuracy": acc, "f1": f1}

!pip install --quiet accelerate -U

#!pip install --quiet transformers[torch] -U

from transformers import Trainer, TrainingArguments

import transformers
from transformers import Trainer, TrainingArguments


batch_size = 64
logging_steps = len(emotions_encoded) // batch_size
model_name = f"{model_ckpt}-finetuned-emotion"
training_args = TrainingArguments(output_dir=model_name, num_train_epochs=2,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01, evaluation_strategy="epoch",
                                  disable_tqdm=False, logging_steps=logging_steps,
                                  push_to_hub=False, log_level="error")

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=train_dataset, eval_dataset=val_dataset,
                  tokenizer=tokenizer)

trainer.train()

