# -*- coding: utf-8 -*-
"""GO_Preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15tY_KE7kr-ACJFmHaxeyw-Cuhq5BHUD8
"""

import numpy as np
import pandas as pd
from google.colab import drive
import requests
import os

"""Downloading the Dataset"""

base_url = 'https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/'
file_names = ['goemotions_1.csv', 'goemotions_2.csv', 'goemotions_3.csv']
base_save_path = 'data/dataset/'


os.makedirs(base_save_path, exist_ok=True)

for file_name in file_names:

    url = base_url + file_name
    save_path = os.path.join(base_save_path, file_name)

    response = requests.get(url)
    response.raise_for_status()

  # Save the file
    with open(save_path, 'wb') as f:
        f.write(response.content)

    print(f'File downloaded and saved at {save_path}')

df1 = pd.read_csv('data/dataset/goemotions_1.csv')
df2 = pd.read_csv('data/dataset/goemotions_2.csv')
df3 = pd.read_csv('data/dataset/goemotions_3.csv')

print(df1.columns)

df_combined = pd.concat([df1,df2,df3], ignore_index=True)
print("DF1 shape:", df1.shape)
print("DF2 shape:",df2.shape)
print("DF3 shape:",df3.shape)
print("combined DF:",df_combined.shape)

features = df_combined.columns.tolist()
print(features)

emotion_columns = ['admiration', 'amusement', 'anger', 'annoyance', 'approval',
                   'caring', 'confusion', 'curiosity', 'desire', 'disappointment',
                   'disapproval', 'disgust', 'embarrassment', 'excitement',
                   'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness',
                   'optimism', 'pride', 'realization', 'relief', 'remorse',
                   'sadness', 'surprise', 'neutral']

# Group by 'text_id' to aggregate ratings for the same text
grouped = df_combined.groupby('id')

# List to store DataFrames for concatenation
agreed_dfs = []

# Iterate over each group and count '1's in each emotion column
for text_id, group in grouped:
    # Sum up the '1's in each emotion column and check if any are greater than 1
    emotion_counts = group[emotion_columns].sum()
    if any(emotion_counts > 1):
        # If there's agreement on any emotion, append the sum (or an example row) to the list
        group['agreement_count'] = emotion_counts.max()  # Add a column showing max agreement for clarity
        agreed_dfs.append(group)

# Concatenate all the agreed DataFrames into a single DataFrame
if agreed_dfs:
    df_with_agreement = pd.concat(agreed_dfs, ignore_index=True)
else:
    df_with_agreement = pd.DataFrame()  # If no agreement found, return an empty DataFrame

df_with_agreement.head(15)

selected_rows = []

# Iterate over each DataFrame in the list
for df in agreed_dfs:
    # Check the agreement_count in the first row
    if df.iloc[0]['agreement_count'] > 1:
        # Select the first row if the condition is met
        selected_rows.append(df.iloc[[0]])

# Concatenate all selected rows into a new DataFrame
if selected_rows:
    result_df = pd.concat(selected_rows, ignore_index=True)
else:
    result_df = pd.DataFrame()  # Return an empty DataFrame if no rows meet the criteria

result_df.head(20)

print(len(result_df))
import matplotlib.pyplot as plt
value_counts = result_df['agreement_count'].value_counts()
value_counts.plot(kind='bar')
plt.title('Histogram of agreement')
plt.xlabel('Classification')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

result_df.columns

result_df.drop(['id', 'author', 'subreddit', 'link_id', 'parent_id',
                  'created_utc', 'rater_id','agreement_count'], axis=1, inplace=True)
print("Shape:", result_df.shape)

print(result_df.shape)
result_df.head(10)

import matplotlib.pyplot as plt
value_counts = result_df['example_very_unclear'].value_counts()
value_counts.plot(kind='bar')
plt.title('Histogram of True/False Classifications')
plt.xlabel('Classification')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

result_df = result_df.loc[result_df['example_very_unclear'] != True]
print("Shape:", result_df.shape)

result_df.drop_duplicates(inplace=True)
print("Shape:", result_df.shape)

result_df.drop('example_very_unclear', axis=1, inplace=True)
print("Shape:", result_df.shape)

column_names = result_df.columns

def find_emotion_label(row):
    for col in column_names:
        if row[col] == 1:
            return col
    return np.nan
# Apply the function to each row to create a new 'emotion_label' column
result_df['emotion_label'] = result_df.apply(find_emotion_label, axis=1)

emotion_counts = result_df['emotion_label'].value_counts()


plt.figure(figsize=(10, 6))
emotion_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Frequency of Emotion Labels')
plt.xlabel('Emotion Label')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

print('Result:', len(result_df))
emotion_counts



rebalanced_df = pd.DataFrame()
rebalanced_df = result_df

rebalanced_df.loc[rebalanced_df['emotion_label'].isin(['admiration', 'relief']), 'emotion_label'] = 'gratitude'

rebalanced_df.loc[rebalanced_df['emotion_label'].isin(['annoyance', 'disgust','disapproval']),'emotion_label'] = 'anger'

rebalanced_df.loc[rebalanced_df['emotion_label'].isin(['optimism', 'caring','excitement', 'amusement', 'love']), 'emotion_label'] = 'joy'

rebalanced_df.loc[rebalanced_df['emotion_label'].isin(['grief', 'remorse', 'disappointment']), 'emotion_label'] = 'sadness'

rebalanced_df.loc[rebalanced_df['emotion_label'].isin(['nervousness', 'embarrassment', 'confusion']), 'emotion_label'] = 'fear'

excluded = ['approval', 'curiosity', 'realization', 'surprise','desire','pride']

rebalanced_df = rebalanced_df[~rebalanced_df['emotion_label'].isin(excluded)]

emotion_counts = rebalanced_df['emotion_label'].value_counts()
print(emotion_counts)

target_size = 8790
neutral_df = rebalanced_df[rebalanced_df['emotion_label'] == 'neutral']

# Downsample the 'Neutral' entries to the target size
downsampled_neutral_df = neutral_df.sample(n=target_size, random_state=42)

# Combine back with the non-'Neutral' entries
downsampled_df = pd.concat([downsampled_neutral_df, rebalanced_df[rebalanced_df['emotion_label'] != 'neutral']])

# Verify the changes
print(downsampled_df['emotion_label'].value_counts())

from sklearn.utils import resample
# Find the number of samples in the largest class
max_size = downsampled_df['emotion_label'].value_counts().max()

# List of all unique classes
classes = downsampled_df['emotion_label'].unique()

# Upsample each class to the max size
df_balanced = pd.DataFrame()  # Initialize an empty DataFrame to hold the balanced data
for cls in classes:
    df_class = downsampled_df[downsampled_df['emotion_label'] == cls]
    df_class_upsampled = resample(df_class,
                                  replace=True,           # Sample with replacement
                                  n_samples=max_size,     # Match number in majority class
                                  random_state=123)       # Reproducible results
    df_balanced = pd.concat([df_balanced, df_class_upsampled], axis=0)

# Display the new class counts
print(df_balanced['emotion_label'].value_counts())

emotion_counts = df_balanced['emotion_label'].value_counts()

# Create a bar plot
plt.figure(figsize=(10, 6))  # Adjusts the size of the bar plot
emotion_counts.plot(kind='bar', color='skyblue', edgecolor='black')  # Creates a bar plot
plt.title('Frequency of Emotion Labels')
plt.xlabel('Emotion Label')
plt.ylabel('Frequency')
plt.xticks(rotation=45)  # Rotates the x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.6)  # Adds a grid for easier reading
plt.show()

df_clean = pd.DataFrame()
df_clean['text'] = df_balanced['text']
df_clean['emotion_label'] = df_balanced['emotion_label']
df_clean.head(10)



import re

def clean_text(text):
	text = str(text).lower()  # Convert text to lowercase
	text = re.sub("[\[].*?[\]]", "", text)  # Remove text within square brackets
	text = re.sub(r"http\S+", "", text)  # Remove URLs
	text = re.sub(r"[^\w\s]", "", text)  # Remove non-alphanumeric characters (excluding spaces)
	text = re.sub(r"\d+", "", text)  # Remove digits
	return text

df_clean['clean_text'] = df_clean['text'].apply(clean_text)
#df_clean = df_unique.drop(columns=['text'])
df_clean.head(20)

"""Processing functions"""

import spacy
nlp = spacy.load('en_core_web_sm')

def remove_stopwords(text):
    doc = nlp(text)
    text_wo_stopwords = ' '.join([token.text for token in doc if not token.is_stop])
    return text_wo_stopwords

def lemmas_tokens(text):
  doc = nlp(text)
  lemmas = ' '.join([token.lemma_ for token in doc])
  return lemmas

def tokenize(text):
    doc = nlp(text)
    tokens = [token.text for token in doc]
    return tokens

"""Already Preprocessed to the "processed.csv" file."""

# This cell takes a long time to apply lemmatization and Tokenization.
# Upload "processed.csv" file generated to the root of the environment.

df_clean['lemmas'] = df_clean['clean_text'].apply(lemmas_tokens)
df_clean['lemmas'] = df_clean['lemmas'].astype(str)
df_clean['tokens'] = df_clean['lemmas'].apply(tokenize)
file_name = '/content/processed.csv'
df_clean.to_csv(file_name, index=False)

df_preprocessed = pd.read_csv('processed.csv')
df_preprocessed.head(10)

